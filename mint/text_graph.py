import networkx as nx
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import os
import json
from openai import OpenAI
from dotenv import load_dotenv
import numpy as np
from transformers import AutoTokenizer, AutoModel
import torch
import faiss
from .beam_search import BeamSearchPathFinder
import unicodedata
import re
from difflib import SequenceMatcher
from typing import List, Dict

try:
    from mint.helpers import segment_entity_with_vncorenlp
except ImportError:
    try:
        from process_with_beam_search_fixed import segment_entity_with_vncorenlp
    except ImportError:
        segment_entity_with_vncorenlp = None

try:
    from sklearn.metrics.pairwise import cosine_similarity
except ImportError:
    cosine_similarity = None

class TextGraph:
    """
    L·ªõp TextGraph ƒë·ªÉ x√¢y d·ª±ng v√† ph√¢n t√≠ch ƒë·ªì th·ªã vƒÉn b·∫£n t·ª´ context v√† claim
    
    ƒê·ªì th·ªã bao g·ªìm c√°c lo·∫°i node:
    - Word nodes: ch·ª©a t·ª´ng t·ª´ trong context v√† claim
    - Sentence nodes: c√°c c√¢u trong context  
    - Claim node: gi√° tr·ªã claim
    """
    
    def __init__(self):
        self.graph = nx.Graph()
        self.word_nodes = {}
        self.sentence_nodes = {}
        self.claim_node = None
        self.entity_nodes = {}  # Th√™m dictionary ƒë·ªÉ qu·∫£n l√Ω entity nodes
        self.claim_entities = set()  # ‚úÖ M·ªöI: L∆∞u claim entities ƒë·ªÉ scoring
        
        # POS tag filtering configuration
        self.enable_pos_filtering = True  # M·∫∑c ƒë·ªãnh b·∫≠t ƒë·ªÉ gi·∫£m nhi·ªÖu
        self.important_pos_tags = {
            'N',    # Danh t·ª´ th∆∞·ªùng
            'Np',   # Danh t·ª´ ri√™ng
            'V',    # ƒê·ªông t·ª´
            'A',    # T√≠nh t·ª´
            'Nc',   # Danh t·ª´ ch·ªâ ng∆∞·ªùi
            'M',    # S·ªë t·ª´
            'R',    # Tr·∫°ng t·ª´ (c√≥ th·ªÉ tranh lu·∫≠n)
            'P'     # ƒê·∫°i t·ª´ (c√≥ th·ªÉ tranh lu·∫≠n)
        }
        
        # Load environment variables
        load_dotenv()
        self.openai_client = None
        self._init_openai_client()
        
        # Semantic similarity components
        self.phobert_tokenizer = None
        self.phobert_model = None
        self.word_embeddings = {}  # Cache embeddings
        self.embedding_dim = 768  # PhoBERT base dimension (full dimension - no PCA)
        self.faiss_index = None
        self.word_to_index = {}  # Mapping t·ª´ word -> index trong faiss
        self.index_to_word = {}  # Mapping ng∆∞·ª£c l·∫°i
        
        # Semantic similarity parameters (optimized for full embeddings)
        self.similarity_threshold = 0.85
        self.top_k_similar = 5
        
        self._init_phobert_model()
    
    def set_pos_filtering(self, enable=True, custom_pos_tags=None):
        """
        C·∫•u h√¨nh l·ªçc t·ª´ lo·∫°i cho word nodes
        
        Args:
            enable (bool): B·∫≠t/t·∫Øt t√≠nh nƒÉng l·ªçc t·ª´ lo·∫°i
            custom_pos_tags (set): T·∫≠p h·ª£p c√°c t·ª´ lo·∫°i mu·ªën gi·ªØ l·∫°i (n·∫øu None th√¨ d√πng m·∫∑c ƒë·ªãnh)
        """
        self.enable_pos_filtering = enable
        if custom_pos_tags is not None:
            self.important_pos_tags = set(custom_pos_tags)
    
    def is_important_word(self, word, pos_tag):
        """
        Ki·ªÉm tra xem t·ª´ c√≥ quan tr·ªçng hay kh√¥ng d·ª±a tr√™n t·ª´ lo·∫°i
        
        Args:
            word (str): T·ª´ c·∫ßn ki·ªÉm tra
            pos_tag (str): T·ª´ lo·∫°i c·ªßa t·ª´
            
        Returns:
            bool: True n·∫øu t·ª´ quan tr·ªçng v√† n√™n t·∫°o word node
        """
        # N·∫øu kh√¥ng b·∫≠t l·ªçc t·ª´ lo·∫°i, t·∫•t c·∫£ t·ª´ ƒë·ªÅu quan tr·ªçng
        if not self.enable_pos_filtering:
            return True
            
        # Ki·ªÉm tra t·ª´ lo·∫°i c√≥ trong danh s√°ch quan tr·ªçng kh√¥ng
        return pos_tag in self.important_pos_tags
    
    def add_word_node(self, word, pos_tag=None, lemma=None):
        """Th√™m word node v√†o ƒë·ªì th·ªã (c√≥ th·ªÉ l·ªçc theo t·ª´ lo·∫°i)"""
        # Ki·ªÉm tra xem t·ª´ c√≥ quan tr·ªçng kh√¥ng
        if not self.is_important_word(word, pos_tag):
            return None  # Kh√¥ng t·∫°o node cho t·ª´ kh√¥ng quan tr·ªçng
            
        if word not in self.word_nodes:
            node_id = f"word_{len(self.word_nodes)}"
            self.word_nodes[word] = node_id
            self.graph.add_node(node_id, 
                              type="word", 
                              text=word, 
                              pos=pos_tag, 
                              lemma=lemma)
        return self.word_nodes[word]
    
    def add_sentence_node(self, sentence_id, sentence_text):
        """Th√™m sentence node v√†o ƒë·ªì th·ªã"""
        node_id = f"sentence_{sentence_id}"
        self.sentence_nodes[sentence_id] = node_id
        self.graph.add_node(node_id, 
                          type="sentence", 
                          text=sentence_text)
        return node_id
    
    def add_claim_node(self, claim_text):
        """Th√™m claim node v√†o ƒë·ªì th·ªã"""
        self.claim_node = "claim_0"
        self.graph.add_node(self.claim_node, 
                          type="claim", 
                          text=claim_text)
        return self.claim_node
    
    def connect_word_to_sentence(self, word_node, sentence_node):
        """K·∫øt n·ªëi word v·ªõi sentence"""
        self.graph.add_edge(word_node, sentence_node, relation="belongs_to", edge_type="structural")
    
    def connect_word_to_claim(self, word_node, claim_node):
        """K·∫øt n·ªëi word v·ªõi claim"""
        self.graph.add_edge(word_node, claim_node, relation="belongs_to", edge_type="structural")
    
    def connect_dependency(self, dependent_word_node, head_word_node, dep_label):
        """K·∫øt n·ªëi dependency gi·ªØa hai t·ª´"""
        self.graph.add_edge(dependent_word_node, head_word_node, 
                          relation=dep_label, edge_type="dependency")
    
    def build_from_vncorenlp_output(self, context_sentences, claim_text, claim_sentences):
        """X√¢y d·ª±ng ƒë·ªì th·ªã t·ª´ k·∫øt qu·∫£ py_vncorenlp"""
        
        # Th√™m claim node
        claim_node = self.add_claim_node(claim_text)
        
        # X·ª≠ l√Ω c√°c c√¢u trong context (context_sentences l√† dict)
        for sent_idx, sentence_tokens in context_sentences.items():
            sentence_text = " ".join([token["wordForm"] for token in sentence_tokens])
            sentence_node = self.add_sentence_node(sent_idx, sentence_text)
            
            # Dictionary ƒë·ªÉ map index -> word_node_id cho vi·ªác t·∫°o dependency links
            token_index_to_node = {}
            
            # Th√™m c√°c word trong sentence
            for token in sentence_tokens:
                word = token["wordForm"]
                pos_tag = token.get("posTag", "")
                lemma = token.get("lemma", "")
                token_index = token.get("index", 0)
                
                word_node = self.add_word_node(word, pos_tag, lemma)
                
                # Ch·ªâ t·∫°o k·∫øt n·ªëi n·∫øu word_node ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng (kh√¥ng b·ªã l·ªçc)
                if word_node is not None:
                    self.connect_word_to_sentence(word_node, sentence_node)
                    # L∆∞u mapping ƒë·ªÉ t·∫°o dependency links sau
                    token_index_to_node[token_index] = word_node
            
            # T·∫°o dependency connections gi·ªØa c√°c t·ª´ trong c√¢u
            for token in sentence_tokens:
                token_index = token.get("index", 0)
                head_index = token.get("head", 0)
                dep_label = token.get("depLabel", "")
                
                # Ch·ªâ t·∫°o dependency n·∫øu c·∫£ dependent v√† head ƒë·ªÅu t·ªìn t·∫°i trong mapping
                if (head_index > 0 and 
                    token_index in token_index_to_node and 
                    head_index in token_index_to_node):
                    dependent_node = token_index_to_node[token_index]
                    head_node = token_index_to_node[head_index]
                    self.connect_dependency(dependent_node, head_node, dep_label)
        
        # X·ª≠ l√Ω c√°c word trong claim (claim_sentences c≈©ng l√† dict)
        for sent_idx, sentence_tokens in claim_sentences.items():
            # Dictionary ƒë·ªÉ map index -> word_node_id cho claim
            claim_token_index_to_node = {}
            
            # Th√™m words
            for token in sentence_tokens:
                word = token["wordForm"]
                pos_tag = token.get("posTag", "")
                lemma = token.get("lemma", "")
                token_index = token.get("index", 0)
                
                word_node = self.add_word_node(word, pos_tag, lemma)
                
                # Ch·ªâ t·∫°o k·∫øt n·ªëi n·∫øu word_node ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng (kh√¥ng b·ªã l·ªçc)
                if word_node is not None:
                    self.connect_word_to_claim(word_node, claim_node)
                    # L∆∞u mapping cho dependency links
                    claim_token_index_to_node[token_index] = word_node
            
            # T·∫°o dependency connections trong claim
            for token in sentence_tokens:
                token_index = token.get("index", 0)
                head_index = token.get("head", 0)
                dep_label = token.get("depLabel", "")
                
                # Ch·ªâ t·∫°o dependency n·∫øu c·∫£ dependent v√† head ƒë·ªÅu t·ªìn t·∫°i trong mapping
                if (head_index > 0 and 
                    token_index in claim_token_index_to_node and 
                    head_index in claim_token_index_to_node):
                    dependent_node = claim_token_index_to_node[token_index]
                    head_node = claim_token_index_to_node[head_index]
                    self.connect_dependency(dependent_node, head_node, dep_label)
    
    def get_statistics(self):
        """Th·ªëng k√™ c∆° b·∫£n v·ªÅ ƒë·ªì th·ªã"""
        word_count = len([n for n in self.graph.nodes() if self.graph.nodes[n]['type'] == 'word'])
        sentence_count = len([n for n in self.graph.nodes() if self.graph.nodes[n]['type'] == 'sentence'])
        claim_count = len([n for n in self.graph.nodes() if self.graph.nodes[n]['type'] == 'claim'])
        entity_count = len([n for n in self.graph.nodes() if self.graph.nodes[n]['type'] == 'entity'])
        
        return {
            "total_nodes": self.graph.number_of_nodes(),
            "total_edges": self.graph.number_of_edges(),
            "word_nodes": word_count,
            "sentence_nodes": sentence_count,
            "claim_nodes": claim_count,
            "entity_nodes": entity_count
        }
    
    def get_shared_words(self):
        """T√¨m c√°c t·ª´ xu·∫•t hi·ªán c·∫£ trong context v√† claim"""
        shared_words = []
        
        for word_node_id in self.word_nodes.values():
            # Ki·ªÉm tra xem word node c√≥ k·∫øt n·ªëi v·ªõi c·∫£ sentence nodes v√† claim node kh√¥ng
            neighbors = list(self.graph.neighbors(word_node_id))
            has_sentence_connection = any(
                self.graph.nodes[neighbor]['type'] == 'sentence' for neighbor in neighbors
            )
            has_claim_connection = any(
                self.graph.nodes[neighbor]['type'] == 'claim' for neighbor in neighbors
            )
            
            if has_sentence_connection and has_claim_connection:
                word_text = self.graph.nodes[word_node_id]['text']
                pos_tag = self.graph.nodes[word_node_id]['pos']
                shared_words.append({
                    'word': word_text,
                    'pos': pos_tag,
                    'node_id': word_node_id
                })
        
        return shared_words
    
    def get_word_frequency(self):
        """ƒê·∫øm t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa t·ª´ng t·ª´"""
        word_freq = {}
        for word_node_id in self.word_nodes.values():
            word_text = self.graph.nodes[word_node_id]['text']
            word_freq[word_text] = word_freq.get(word_text, 0) + 1
        return word_freq
    
    def get_dependency_statistics(self):
        """Th·ªëng k√™ v·ªÅ c√°c m·ªëi quan h·ªá dependency"""
        dependency_edges = [
            (u, v, data) for u, v, data in self.graph.edges(data=True) 
            if data.get('edge_type') == 'dependency'
        ]
        
        # ƒê·∫øm c√°c lo·∫°i dependency
        dep_types = {}
        for u, v, data in dependency_edges:
            dep_label = data.get('relation', 'unknown')
            dep_types[dep_label] = dep_types.get(dep_label, 0) + 1
        
        return {
            "total_dependency_edges": len(dependency_edges),
            "dependency_types": dep_types,
            "most_common_dependencies": sorted(dep_types.items(), key=lambda x: x[1], reverse=True)[:10]
        }
    
    def get_word_dependencies(self, word):
        """L·∫•y t·∫•t c·∫£ dependencies c·ªßa m·ªôt t·ª´"""
        if word not in self.word_nodes:
            return {"dependents": [], "heads": []}
        
        word_node_id = self.word_nodes[word]
        dependents = []
        heads = []
        
        for neighbor in self.graph.neighbors(word_node_id):
            edge_data = self.graph.edges[word_node_id, neighbor]
            if edge_data.get('edge_type') == 'dependency':
                dep_relation = edge_data.get('relation', '')
                neighbor_word = self.graph.nodes[neighbor]['text']
                
                # Ki·ªÉm tra xem word_node_id l√† head hay dependent
                # Trong NetworkX undirected graph, c·∫ßn ki·ªÉm tra h∆∞·ªõng d·ª±a tr√™n semantic
                # Gi·∫£ s·ª≠ edge ƒë∆∞·ª£c t·∫°o t·ª´ dependent -> head
                if (word_node_id, neighbor) in self.graph.edges():
                    heads.append({"word": neighbor_word, "relation": dep_relation})
                else:
                    dependents.append({"word": neighbor_word, "relation": dep_relation})
        
        return {"dependents": dependents, "heads": heads}
    
    def get_detailed_statistics(self):
        """Th·ªëng k√™ chi ti·∫øt v·ªÅ ƒë·ªì th·ªã"""
        basic_stats = self.get_statistics()
        shared_words = self.get_shared_words()
        word_freq = self.get_word_frequency()
        dep_stats = self.get_dependency_statistics()
        semantic_stats = self.get_semantic_statistics()
        
        # T√¨m t·ª´ xu·∫•t hi·ªán nhi·ªÅu nh·∫•t
        most_frequent_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        
        # T√≠nh t·ªïng edges theo lo·∫°i
        structural_edges = len([
            (u, v) for u, v, data in self.graph.edges(data=True) 
            if data.get('edge_type') == 'structural'
        ])
        
        entity_structural_edges = len([
            (u, v) for u, v, data in self.graph.edges(data=True) 
            if data.get('edge_type') == 'entity_structural'
        ])
        
        # Th·ªëng k√™ entity
        entity_list = [
            {
                'name': self.graph.nodes[node_id]['text'],
                'type': self.graph.nodes[node_id].get('entity_type', 'ENTITY'),
                'connected_sentences': len([
                    neighbor for neighbor in self.graph.neighbors(node_id) 
                    if self.graph.nodes[neighbor]['type'] == 'sentence'
                ])
            }
            for node_id in self.graph.nodes() 
            if self.graph.nodes[node_id]['type'] == 'entity'
        ]
        
        return {
            **basic_stats,
            "shared_words_count": len(shared_words),
            "shared_words": shared_words,
            "unique_words": len(word_freq),
            "most_frequent_words": most_frequent_words,
            "average_words_per_sentence": basic_stats['word_nodes'] / max(basic_stats['sentence_nodes'], 1),
            "dependency_statistics": dep_stats,
            "structural_edges": structural_edges,
            "dependency_edges": dep_stats["total_dependency_edges"],
            "entity_structural_edges": entity_structural_edges,
            "entities": entity_list,
            "unique_entities": len(entity_list),
            "semantic_statistics": semantic_stats,
            "semantic_edges": semantic_stats["total_semantic_edges"]
        }
    
    def visualize(self, figsize=(15, 10), show_dependencies=True, show_semantic=True):
        """V·∫Ω ƒë·ªì th·ªã v·ªõi ph√¢n bi·ªát structural, dependency, entity v√† semantic edges"""
        plt.figure(figsize=figsize)
        
        # ƒê·ªãnh nghƒ©a m√†u s·∫Øc cho c√°c lo·∫°i node
        node_colors = []
        node_sizes = []
        for node in self.graph.nodes():
            node_type = self.graph.nodes[node]['type']
            if node_type == 'word':
                node_colors.append('lightblue')
                node_sizes.append(200)
            elif node_type == 'sentence':
                node_colors.append('lightgreen')
                node_sizes.append(500)
            elif node_type == 'claim':
                node_colors.append('lightcoral')
                node_sizes.append(600)
            elif node_type == 'entity':
                node_colors.append('gold')
                node_sizes.append(400)
        
        # T·∫°o layout
        pos = nx.spring_layout(self.graph, k=2, iterations=100)
        
        # Ph√¢n chia edges theo lo·∫°i
        structural_edges = []
        dependency_edges = []
        entity_edges = []
        semantic_edges = []
        
        for u, v, data in self.graph.edges(data=True):
            edge_type = data.get('edge_type', 'structural')
            if edge_type == 'structural':
                structural_edges.append((u, v))
            elif edge_type == 'dependency':
                dependency_edges.append((u, v))
            elif edge_type == 'entity_structural':
                entity_edges.append((u, v))
            elif edge_type == 'semantic':
                semantic_edges.append((u, v))
        
        # V·∫Ω nodes
        nx.draw_networkx_nodes(self.graph, pos, 
                             node_color=node_colors,
                             node_size=node_sizes,
                             alpha=0.8)
        
        # V·∫Ω structural edges (word -> sentence/claim)
        if structural_edges:
            nx.draw_networkx_edges(self.graph, pos,
                                 edgelist=structural_edges,
                                 edge_color='gray',
                                 style='-',
                                 width=1,
                                 alpha=0.6)
        
        # V·∫Ω entity edges (entity -> sentence)
        if entity_edges:
            nx.draw_networkx_edges(self.graph, pos,
                                 edgelist=entity_edges,
                                 edge_color='orange',
                                 style='-',
                                 width=2,
                                 alpha=0.7)
        
        # V·∫Ω semantic edges (word -> word)
        if show_semantic and semantic_edges:
            nx.draw_networkx_edges(self.graph, pos,
                                 edgelist=semantic_edges,
                                 edge_color='purple',
                                 style=':',
                                 width=1.5,
                                 alpha=0.8)
        
        # V·∫Ω dependency edges (word -> word)
        if show_dependencies and dependency_edges:
            nx.draw_networkx_edges(self.graph, pos,
                                 edgelist=dependency_edges,
                                 edge_color='red',
                                 style='--',
                                 width=0.8,
                                 alpha=0.7,
                                 arrows=True,
                                 arrowsize=10)
        
        # Th√™m legend
        legend_elements = [
            mpatches.Patch(color='lightblue', label='Word nodes'),
            mpatches.Patch(color='lightgreen', label='Sentence nodes'),
            mpatches.Patch(color='lightcoral', label='Claim node'),
            mpatches.Patch(color='gold', label='Entity nodes')
        ]
        
        edge_legend = []
        if structural_edges:
            edge_legend.append(plt.Line2D([0], [0], color='gray', label='Structural edges'))
        if entity_edges:
            edge_legend.append(plt.Line2D([0], [0], color='orange', label='Entity edges'))
        if show_semantic and semantic_edges:
            edge_legend.append(plt.Line2D([0], [0], color='purple', linestyle=':', label='Semantic edges'))
        if show_dependencies and dependency_edges:
            edge_legend.append(plt.Line2D([0], [0], color='red', linestyle='--', label='Dependency edges'))
        
        legend_elements.extend(edge_legend)
        
        plt.legend(handles=legend_elements, loc='upper right')
        
        title = f"Text Graph: Words, Sentences, Claim, Entities ({len(self.entity_nodes)} entities)"
        if show_semantic and semantic_edges:
            title += f", Semantic ({len(semantic_edges)} edges)"
        if show_dependencies and dependency_edges:
            title += f", Dependencies ({len(dependency_edges)} edges)"
        
        plt.title(title)
        plt.axis('off')
        plt.tight_layout()
        plt.show()
    
    def visualize_dependencies_only(self, figsize=(12, 8)):
        """V·∫Ω ch·ªâ dependency graph gi·ªØa c√°c t·ª´"""
        # T·∫°o subgraph ch·ªâ v·ªõi word nodes v√† dependency edges
        word_nodes = [n for n in self.graph.nodes() if self.graph.nodes[n]['type'] == 'word']
        dependency_edges = [
            (u, v) for u, v, data in self.graph.edges(data=True) 
            if data.get('edge_type') == 'dependency'
        ]
        
        if not dependency_edges:
            print("Kh√¥ng c√≥ dependency edges ƒë·ªÉ v·∫Ω!")
            return
        
        # T·∫°o subgraph
        subgraph = self.graph.edge_subgraph(dependency_edges).copy()
        
        plt.figure(figsize=figsize)
        
        # Layout cho dependency graph
        pos = nx.spring_layout(subgraph, k=1.5, iterations=100)
        
        # V·∫Ω nodes v·ªõi labels
        nx.draw_networkx_nodes(subgraph, pos, 
                             node_color='lightblue',
                             node_size=300,
                             alpha=0.8)
        
        # V·∫Ω edges v·ªõi labels
        nx.draw_networkx_edges(subgraph, pos,
                             edge_color='red',
                             style='-',
                             width=1.5,
                             alpha=0.7,
                             arrows=True,
                             arrowsize=15)
        
        # Th√™m node labels (t·ª´)
        node_labels = {node: self.graph.nodes[node]['text'][:10] 
                      for node in subgraph.nodes()}
        nx.draw_networkx_labels(subgraph, pos, node_labels, font_size=8)
        
        # Th√™m edge labels (dependency relations)
        edge_labels = {(u, v): data.get('relation', '') 
                      for u, v, data in subgraph.edges(data=True)}
        nx.draw_networkx_edge_labels(subgraph, pos, edge_labels, font_size=6)
        
        plt.title(f"Dependency Graph ({len(dependency_edges)} dependencies)")
        plt.axis('off')
        plt.tight_layout()
        plt.show()
    
    def save_graph(self, filepath):
        """L∆∞u ƒë·ªì th·ªã v√†o file"""
        # ƒê·∫£m b·∫£o l∆∞u file v√†o th∆∞ m·ª•c g·ªëc c·ªßa project
        if not os.path.isabs(filepath):
            # L·∫•y th∆∞ m·ª•c cha c·ªßa th∆∞ m·ª•c mint
            project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            filepath = os.path.join(project_root, filepath)
        
        # T·∫°o m·ªôt b·∫£n copy c·ªßa graph ƒë·ªÉ x·ª≠ l√Ω None values
        graph_copy = self.graph.copy()
        
        # X·ª≠ l√Ω None values trong node attributes
        for node_id in graph_copy.nodes():
            node_data = graph_copy.nodes[node_id]
            for key, value in node_data.items():
                if value is None:
                    graph_copy.nodes[node_id][key] = ""
        
        # X·ª≠ l√Ω None values trong edge attributes
        for u, v in graph_copy.edges():
            edge_data = graph_copy.edges[u, v]
            for key, value in edge_data.items():
                if value is None:
                    graph_copy.edges[u, v][key] = ""
        
        nx.write_gexf(graph_copy, filepath)
        print(f"ƒê·ªì th·ªã ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {filepath}")
    
    def load_graph(self, filepath):
        """T·∫£i ƒë·ªì th·ªã t·ª´ file"""
        self.graph = nx.read_gexf(filepath)
        
        # Rebuild node mappings
        self.word_nodes = {}
        self.sentence_nodes = {}
        self.entity_nodes = {}
        self.claim_node = None
        
        for node_id in self.graph.nodes():
            node_data = self.graph.nodes[node_id]
            if node_data['type'] == 'word':
                self.word_nodes[node_data['text']] = node_id
            elif node_data['type'] == 'sentence':
                # Extract sentence index from node_id
                sent_idx = int(node_id.split('_')[1])
                self.sentence_nodes[sent_idx] = node_id
            elif node_data['type'] == 'claim':
                self.claim_node = node_id
            elif node_data['type'] == 'entity':
                self.entity_nodes[node_data['text']] = node_id
        
        print(f"ƒê·ªì th·ªã ƒë√£ ƒë∆∞·ª£c t·∫£i t·ª´: {filepath}")
    
    def export_to_json(self):
        """Xu·∫•t ƒë·ªì th·ªã ra ƒë·ªãnh d·∫°ng JSON ƒë·ªÉ d·ªÖ d√†ng ph√¢n t√≠ch"""
        graph_data = {
            "nodes": [],
            "edges": [],
            "statistics": self.get_detailed_statistics()
        }
        
        # Export nodes
        for node_id in self.graph.nodes():
            node_data = self.graph.nodes[node_id]
            graph_data["nodes"].append({
                "id": node_id,
                "type": node_data["type"],
                "text": node_data["text"],
                "pos": node_data.get("pos", ""),
                "lemma": node_data.get("lemma", "")
            })
        
        # Export edges
        for edge in self.graph.edges():
            edge_data = self.graph.edges[edge]
            graph_data["edges"].append({
                "source": edge[0],
                "target": edge[1],
                "relation": edge_data.get("relation", ""),
                "edge_type": edge_data.get("edge_type", "")
            })
        
        return json.dumps(graph_data, ensure_ascii=False, indent=2)
    
    def _init_openai_client(self):
        """Kh·ªüi t·∫°o OpenAI client"""
        try:
            # Try multiple key names for backward compatibility
            api_key = os.getenv('OPENAI_KEY') or os.getenv('OPENAI_API_KEY')
            if api_key and api_key != 'your_openai_api_key_here':
                self.openai_client = OpenAI(api_key=api_key)
                # Only print once globally
                if not hasattr(TextGraph, '_openai_initialized'):
                    print("‚úÖ OpenAI client initialized")
                    TextGraph._openai_initialized = True
            else:
                if not hasattr(self, '_openai_warning_shown'):
                    print("Warning: OPENAI_KEY ho·∫∑c OPENAI_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y trong .env file.")
                    self._openai_warning_shown = True
        except Exception as e:
            print(f"L·ªói khi kh·ªüi t·∫°o OpenAI client: {e}")
    
    def add_entity_node(self, entity_name, entity_type="ENTITY"):
        """Th√™m entity node v√†o ƒë·ªì th·ªã"""
        if entity_name not in self.entity_nodes:
            node_id = f"entity_{len(self.entity_nodes)}"
            self.entity_nodes[entity_name] = node_id
            self.graph.add_node(node_id, 
                              type="entity", 
                              text=entity_name,
                              entity_type=entity_type)
        return self.entity_nodes[entity_name]
    
    def connect_entity_to_sentence(self, entity_node, sentence_node):
        """K·∫øt n·ªëi entity v·ªõi sentence"""
        self.graph.add_edge(entity_node, sentence_node, relation="mentioned_in", edge_type="entity_structural")
    
    def _update_openai_model(self, model=None, temperature=None, max_tokens=None):
        """Update OpenAI model parameters"""
        if model:
            self.openai_model = model
        if temperature is not None:
            self.openai_temperature = temperature  
        if max_tokens is not None:
            self.openai_max_tokens = max_tokens
    
    def extract_entities_with_openai(self, context_text):
        """Tr√≠ch xu·∫•t entities t·ª´ context b·∫±ng OpenAI GPT-4o-mini"""
        if not self.openai_client:
            print("OpenAI client ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o. Kh√¥ng th·ªÉ tr√≠ch xu·∫•t entities.")
            return []
        
        try:
            # Prompt ƒë·ªÉ tr√≠ch xu·∫•t entities bao g·ªìm ng√†y th√°ng v√† s·ªë l∆∞·ª£ng quan tr·ªçng
            prompt = f"""
B·∫°n l√† m·ªôt chuy√™n gia tr√≠ch xu·∫•t th√¥ng tin cho h·ªá th·ªëng fact-checking. H√£y tr√≠ch xu·∫•t t·∫•t c·∫£ c√°c th·ª±c th·ªÉ quan tr·ªçng t·ª´ vƒÉn b·∫£n sau, bao g·ªìm C·∫¢ NG√ÄY TH√ÅNG v√† S·ªê L∆Ø·ª¢NG QUAN TR·ªåNG.
Quan tr·ªçng, ch·ªâ l·∫•y nh·ªØng t·ª´ c√≥ trong vƒÉn b·∫£n, kh√¥ng l·∫•y nh·ªØng t·ª´ kh√¥ng c√≥ trong vƒÉn b·∫£n. N·∫øu tr√≠ch xu·∫•t ƒë∆∞·ª£c c√°c t·ª´ th√¨ ph·∫£i ƒë·ªÉ n√≥ gi·ªëng y nh∆∞ trong vƒÉn b·∫£n kh√¥ng ƒë∆∞·ª£c thay ƒë·ªïi.

NGUY√äN T·∫ÆC TR√çCH XU·∫§T:
- L·∫•y T√äN TH·ª∞C TH·ªÇ THU·∫¶N T√öY + NG√ÄY TH√ÅNG + S·ªê L∆Ø·ª¢NG QUAN TR·ªåNG
- Lo·∫°i b·ªè t·ª´ ph√¢n lo·∫°i kh√¥ng c·∫ßn thi·∫øt: "con", "chi·∫øc", "c√°i", "ng∆∞·ªùi" (tr·ª´ khi l√† ph·∫ßn c·ªßa t√™n ri√™ng)
- Gi·ªØ nguy√™n s·ªë ƒëo l∆∞·ªùng c√≥ √Ω nghƒ©a th·ª±c t·∫ø
Y√äU C·∫¶U:
Ch·ªâ l·∫•y nh·ªØng t·ª´/c·ª•m t·ª´ xu·∫•t hi·ªán trong vƒÉn b·∫£n, gi·ªØ nguy√™n ch√≠nh t·∫£, kh√¥ng t·ª± th√™m ho·∫∑c s·ª≠a ƒë·ªïi.
V·ªõi m·ªói th·ª±c th·ªÉ, ch·ªâ l·∫•y m·ªôt l·∫ßn (kh√¥ng l·∫∑p l·∫°i), k·ªÉ c·∫£ xu·∫•t hi·ªán nhi·ªÅu l·∫ßn trong vƒÉn b·∫£n.
N·∫øu th·ª±c th·ªÉ l√† m·ªôt ph·∫ßn c·ªßa c·ª•m danh t·ª´ l·ªõn h∆°n (v√≠ d·ª•: "ƒëo√†n c·ª©u h·ªô Vi·ªát Nam"), h√£y tr√≠ch xu·∫•t c·∫£ c·ª•m danh t·ª´ l·ªõn ("ƒëo√†n c·ª©u h·ªô Vi·ªát Nam") v√† th·ª±c th·ªÉ nh·ªè b√™n trong ("Vi·ªát Nam").
Kh√¥ng b·ªè s√≥t th·ª±c th·ªÉ ch·ªâ v√¨ n√≥ n·∫±m trong c·ª•m t·ª´ kh√°c ho·∫∑c l√† m·ªôt ph·∫ßn c·ªßa t√™n d√†i.

C√°c lo·∫°i th·ª±c th·ªÉ C·∫¶N tr√≠ch xu·∫•t:
1. **T√™n lo√†i/sinh v·∫≠t**: "Patagotitan mayorum", "titanosaur", "voi ch√¢u Phi"
2. **ƒê·ªãa danh**: "Argentina", "London", "Neuquen", "TP.HCM", "Qu·∫≠n 6"
3. **ƒê·ªãa danh k·∫øt h·ª£p**: "B·∫£o t√†ng L·ªãch s·ª≠ t·ª± nhi√™n London", "Nh√† m√°y n∆∞·ªõc T√¢n Hi·ªáp"
4. **T√™n ri√™ng ng∆∞·ªùi**: "Nguy·ªÖn VƒÉn A", "Ph·∫°m VƒÉn Ch√≠nh", "Sinead Marron"
5. **T·ªï ch·ª©c**: "B·∫£o t√†ng L·ªãch s·ª≠ t·ª± nhi√™n", "SAWACO", "Microsoft", "PLO"
6. **S·∫£n ph·∫©m/c√¥ng ngh·ªá**: "iPhone", "ChatGPT", "PhoBERT", "d·ªãch v·ª• c·∫•p n∆∞·ªõc"

7. **NG√ÄY TH√ÅNG & TH·ªúI GIAN QUAN TR·ªåNG**:
   - NƒÉm: "2010", "2017", "2022"
   - Ng√†y th√°ng: "25-3", "15/4/2023", "ng√†y 10 th√°ng 5"
   - Gi·ªù c·ª• th·ªÉ: "22 gi·ªù", "6h30", "14:30"
   - Kho·∫£ng th·ªùi gian: "t·ª´ 22 gi·ªù ƒë·∫øn 6 gi·ªù", "2-3 ng√†y"

8. **S·ªê L∆Ø·ª¢NG & ƒêO L∆Ø·ªúNG QUAN TR·ªåNG**:
   - K√≠ch th∆∞·ªõc v·∫≠t l√Ω: "37m", "69 t·∫•n", "6m", "180cm"
   - S·ªë l∆∞·ª£ng c√≥ √Ω nghƒ©a: "6 con", "12 con", "100 ng∆∞·ªùi"  
   - Gi√° tr·ªã ti·ªÅn t·ªá: "5 tri·ªáu ƒë·ªìng", "$100", "‚Ç¨50"
   - T·ª∑ l·ªá ph·∫ßn trƒÉm: "80%", "15%"
   - Nhi·ªát ƒë·ªô: "25¬∞C", "100 ƒë·ªô"

KH√îNG l·∫•y (s·ªë l∆∞·ª£ng kh√¥ng c√≥ √Ω nghƒ©a):
- S·ªë th·ª© t·ª± ƒë∆°n l·∫ª: "1", "2", "3" (tr·ª´ khi l√† nƒÉm ho·∫∑c ƒë·ªãa ch·ªâ)
- T·ª´ ch·ªâ s·ªë l∆∞·ª£ng m∆° h·ªì: "nhi·ªÅu", "√≠t", "v√†i", "m·ªôt s·ªë"
- ƒê∆°n v·ªã ƒëo ƒë∆°n l·∫ª: "m√©t", "t·∫•n", "kg" (ph·∫£i c√≥ s·ªë ƒëi k√®m)

V√≠ d·ª• INPUT: "6 con titanosaur ·ªü Argentina n·∫∑ng 69 t·∫•n, ƒë∆∞·ª£c tr∆∞ng b√†y t·∫°i B·∫£o t√†ng L·ªãch s·ª≠ t·ª± nhi√™n London t·ª´ nƒÉm 2017 l√∫c 14:30"
V√≠ d·ª• OUTPUT: ["titanosaur", "Argentina", "69 t·∫•n", "B·∫£o t√†ng L·ªãch s·ª≠ t·ª± nhi√™n London", "2017", "14:30", "6 con"]

V√≠ d·ª• INPUT: "SAWACO th√¥ng b√°o c√∫p n∆∞·ªõc t·∫°i Qu·∫≠n 6 t·ª´ 22 gi·ªù ng√†y 25-3 ƒë·∫øn 6 gi·ªù ng√†y 26-3"
V√≠ d·ª• OUTPUT: ["SAWACO", "Qu·∫≠n 6", "22 gi·ªù", "25-3", "6 gi·ªù", "26-3"]

Tr·∫£ v·ªÅ JSON array: ["entity1", "entity2", "entity3"]

VƒÉn b·∫£n:
{context_text}
"""

            # Use parameters from CLI if available
            model = getattr(self, 'openai_model', 'gpt-4o-mini')
            temperature = getattr(self, 'openai_temperature', 0.0)
            max_tokens = getattr(self, 'openai_max_tokens', 1000)

            response = self.openai_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=max_tokens
            )
            
            # Parse response
            response_text = response.choices[0].message.content.strip()
            
            # Strip markdown code blocks if present
            if response_text.startswith('```json'):
                response_text = response_text[7:]  # Remove '```json'
            if response_text.startswith('```'):
                response_text = response_text[3:]   # Remove '```'
            if response_text.endswith('```'):
                response_text = response_text[:-3]  # Remove ending '```'
            response_text = response_text.strip()
            
            # C·ªë g·∫Øng parse JSON
            try:
                entities = json.loads(response_text)
                if isinstance(entities, list):
                    # Filter out empty strings and duplicates
                    entities = list(set([entity.strip() for entity in entities if entity.strip()]))
                    print(f"ƒê√£ tr√≠ch xu·∫•t ƒë∆∞·ª£c {len(entities)} entities: {entities}")
                    return entities
                else:
                    print(f"Response kh√¥ng ph·∫£i d·∫°ng list: {response_text}")
                    return []
            except json.JSONDecodeError:
                print(f"Kh√¥ng th·ªÉ parse JSON t·ª´ OpenAI response: {response_text}")
                return []
                
        except Exception as e:
            print(f"L·ªói khi g·ªçi OpenAI API: {e}")
            return []
    
    def normalize_text(self, text):
        if not text:
            return ""
        # Lo·∫°i b·ªè d·∫•u c√¢u, chuy·ªÉn v·ªÅ lower, lo·∫°i b·ªè d·∫•u ti·∫øng Vi·ªát
        text = text.lower()
        text = re.sub(r'[\W_]+', ' ', text)  # b·ªè k√Ω t·ª± kh√¥ng ph·∫£i ch·ªØ/s·ªë
        text = ''.join(c for c in unicodedata.normalize('NFD', text)
                      if unicodedata.category(c) != 'Mn')
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def fuzzy_in(self, entity, claim_text, threshold=0.8):
        # So s√°nh fuzzy: entity c√≥ xu·∫•t hi·ªán g·∫ßn ƒë√∫ng trong claim_text kh√¥ng
        if entity in claim_text:
            return True
        # N·∫øu entity l√† c·ª•m t·ª´, ki·ªÉm tra t·ª´ng t·ª´
        for word in entity.split():
            if word in claim_text:
                return True
        # Fuzzy match to√†n chu·ªói
        ratio = SequenceMatcher(None, entity, claim_text).ratio()
        return ratio >= threshold

    def improved_entity_matching(self, entity, sentence_text, model=None):
        entity_lower = entity.lower()
        sentence_lower = sentence_text.lower()
        # Method 1: Direct matching
        if entity_lower in sentence_lower:
            return True
        # Method 2: Simple space->underscore replacement
        entity_simple_seg = entity.replace(" ", "_").lower()
        if entity_simple_seg in sentence_lower:
            return True
        # Method 3: VnCoreNLP segmentation
        if model and segment_entity_with_vncorenlp:
            try:
                entity_vncorenlp_seg = segment_entity_with_vncorenlp(entity, model).lower()
                if entity_vncorenlp_seg in sentence_lower:
                    return True
            except:
                pass
        # Method 4: Fuzzy matching cho partial matches
        entity_words = entity.split()
        if len(entity_words) > 1:
            all_words_found = True
            for word in entity_words:
                word_variants = [
                    word.lower(),
                    word.replace(" ", "_").lower()
                ]
                word_found = any(variant in sentence_lower for variant in word_variants)
                if not word_found:
                    all_words_found = False
                    break
            if all_words_found:
                return True
        return False

    def add_entities_to_graph(self, entities, context_sentences, model=None):
        """Th√™m entities v√†o graph v√† k·∫øt n·ªëi v·ªõi sentences v·ªõi improved matching. N·∫øu entity xu·∫•t hi·ªán trong claim, k·∫øt n·ªëi v·ªõi claim node."""
        entity_nodes_added = []
        total_connections = 0
        # L·∫•y claim text (n·∫øu c√≥ claim node)
        claim_text = None
        if hasattr(self, 'claim_node') and self.claim_node and self.claim_node in self.graph.nodes:
            claim_text = self.graph.nodes[self.claim_node]['text']
            claim_text_norm = self.normalize_text(claim_text)
        else:
            claim_text_norm = None
        for entity in entities:
            # Th√™m entity node
            entity_node = self.add_entity_node(entity)
            entity_nodes_added.append(entity_node)
            entity_connections = 0
            # T√¨m c√°c sentences c√≥ ch·ª©a entity n√†y
            for sent_idx, sentence_node in self.sentence_nodes.items():
                sentence_text = self.graph.nodes[sentence_node]['text']
                if self.improved_entity_matching(entity, sentence_text, model):
                    self.connect_entity_to_sentence(entity_node, sentence_node)
                    entity_connections += 1
                    total_connections += 1
            # K·∫øt n·ªëi entity v·ªõi claim n·∫øu entity xu·∫•t hi·ªán trong claim (n√¢ng c·∫•p: so s√°nh kh√¥ng d·∫•u, fuzzy)
            # ƒê√°nh d·∫•u entities xu·∫•t hi·ªán trong claim v·ªõi tr·ªçng s·ªë cao h∆°n
            is_claim_entity = False
            if claim_text_norm:
                entity_norm = self.normalize_text(entity)
                if self.fuzzy_in(entity_norm, claim_text_norm, threshold=0.8):
                    self.graph.add_edge(entity_node, self.claim_node, relation="mentioned_in", edge_type="entity_structural")
                    is_claim_entity = True
                    # ƒê√°nh d·∫•u entity n√†y c√≥ trong claim ƒë·ªÉ scoring ∆∞u ti√™n
                    self.graph.nodes[entity_node]['in_claim'] = True
                    self.graph.nodes[entity_node]['claim_importance'] = 2.0  # Tr·ªçng s·ªë cao h∆°n
        # ‚úÖ M·ªöI: N·ªëi tr·ª±c ti·∫øp sentences v·ªõi claim b·∫±ng similarity
        self._connect_sentences_to_claim_by_similarity(claim_text)
        
        print(f"‚úÖ Added {len(entity_nodes_added)} entity nodes to graph")
        return entity_nodes_added
    
    def _connect_sentences_to_claim_by_similarity(self, claim_text):
        """N·ªëi tr·ª±c ti·∫øp sentences v·ªõi claim b·∫±ng text similarity"""
        if not claim_text or not self.sentence_nodes:
            return
        
        claim_words = set(self.normalize_text(claim_text).split())
        connections_added = 0
        
        for sent_idx, sentence_node in self.sentence_nodes.items():
            sentence_text = self.graph.nodes[sentence_node]['text']
            sentence_words = set(self.normalize_text(sentence_text).split())
            
            # T√≠nh word overlap ratio
            overlap = len(claim_words.intersection(sentence_words))
            total_words = len(claim_words.union(sentence_words))
            similarity = overlap / total_words if total_words > 0 else 0.0
            
            # N·ªëi v·ªõi claim n·∫øu similarity ƒë·ªß cao
            if similarity >= 0.15:  # Threshold 15%
                self.graph.add_edge(sentence_node, self.claim_node, 
                                  relation="text_similar", 
                                  edge_type="semantic",
                                  similarity=similarity)
                connections_added += 1
        
        print(f"üîó Connected {connections_added} sentences to claim by text similarity (threshold=0.15)")
    
    def extract_and_add_entities(self, context_text, context_sentences):
        """Ph∆∞∆°ng th·ª©c ch√≠nh ƒë·ªÉ tr√≠ch xu·∫•t v√† th√™m entities v√†o graph"""
        print("ƒêang tr√≠ch xu·∫•t entities t·ª´ OpenAI...")
        entities = self.extract_entities_with_openai(context_text)
        
        if entities:
            print("ƒêang th√™m entities v√†o graph...")
            entity_nodes = self.add_entities_to_graph(entities, context_sentences)
            print(f"Ho√†n th√†nh! ƒê√£ th√™m {len(entity_nodes)} entities v√†o graph.")
            return entity_nodes
        else:
            print("Kh√¥ng c√≥ entities n√†o ƒë∆∞·ª£c tr√≠ch xu·∫•t.")
            return []
    
    def _init_phobert_model(self):
        """Kh·ªüi t·∫°o PhoBERT model"""
        try:
            self.phobert_tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")
            self.phobert_model = AutoModel.from_pretrained("vinai/phobert-base")
            # Only print once globally
            if not hasattr(TextGraph, '_phobert_initialized'):
                print("‚úÖ PhoBERT model initialized")
                TextGraph._phobert_initialized = True
        except Exception as e:
            print(f"L·ªói khi kh·ªüi t·∫°o PhoBERT model: {e}")
    
    def get_word_embeddings(self, words):
        """L·∫•y embeddings c·ªßa c√°c t·ª´"""
        if not self.phobert_tokenizer or not self.phobert_model:
            print("PhoBERT model ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o. Kh√¥ng th·ªÉ l·∫•y embeddings.")
            return None
        
        embeddings = []
        for word in words:
            if word not in self.word_embeddings:
                inputs = self.phobert_tokenizer(word, return_tensors="pt")
                with torch.no_grad():
                    outputs = self.phobert_model(**inputs)
                embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())
                self.word_embeddings[word] = embeddings[-1]
            else:
                embeddings.append(self.word_embeddings[word])
        
        return np.array(embeddings)
    
    def get_similarity(self, word1, word2):
        if not cosine_similarity:
            print("cosine_similarity kh√¥ng kh·∫£ d·ª•ng.")
            return 0.0
        if word1 not in self.word_embeddings or word2 not in self.word_embeddings:
            print(f"T·ª´ '{word1}' ho·∫∑c '{word2}' kh√¥ng c√≥ trong word_embeddings.")
            return 0.0
        embedding1 = self.word_embeddings[word1]
        embedding2 = self.word_embeddings[word2]
        return cosine_similarity([embedding1], [embedding2])[0][0]
    
    def get_similar_words(self, word, top_k=5):
        """T√¨m c√°c t·ª´ c√≥ ƒë·ªô t∆∞∆°ng ƒë·ªìng cao v·ªõi t·ª´ ƒë√£ cho"""
        if word not in self.word_embeddings:
            return []
        
        similarities = []
        for other_word in self.word_embeddings.keys():
            if other_word != word:
                similarity = self.get_similarity(word, other_word)
                similarities.append((other_word, similarity))
        
        similarities.sort(key=lambda x: x[1], reverse=True)
        return [word for word, similarity in similarities[:top_k]]
    
    def get_sentence_embeddings(self, sentences):
        """L·∫•y embeddings c·ªßa c√°c c√¢u"""
        if not self.phobert_tokenizer or not self.phobert_model:
            print("PhoBERT model ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o. Kh√¥ng th·ªÉ l·∫•y embeddings.")
            return None
        
        embeddings = []
        for sentence in sentences:
            inputs = self.phobert_tokenizer(sentence, return_tensors="pt", truncation=True, max_length=256)
            with torch.no_grad():
                outputs = self.phobert_model(**inputs)
            embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())
        
        return np.array(embeddings)
    
    def get_sentence_similarity(self, sentence1, sentence2):
        """T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa hai c√¢u"""
        # L·∫•y embeddings cho c·∫£ 2 c√¢u
        embeddings = self.get_sentence_embeddings([sentence1, sentence2])
        if embeddings is None or len(embeddings) < 2:
            return 0.0
        
        return cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
    
    def build_semantic_similarity_edges(self, use_faiss=True):
        """X√¢y d·ª±ng c√°c c·∫°nh semantic similarity gi·ªØa c√°c t·ª´ (kh√¥ng s·ª≠ d·ª•ng PCA)"""
        print("ƒêang b·∫Øt ƒë·∫ßu x√¢y d·ª±ng semantic similarity edges...")
        
        # L·∫•y t·∫•t c·∫£ word nodes
        word_nodes = [node_id for node_id in self.graph.nodes() 
                     if self.graph.nodes[node_id]['type'] == 'word']
        
        if len(word_nodes) < 2:
            print("C·∫ßn √≠t nh·∫•t 2 word nodes ƒë·ªÉ x√¢y d·ª±ng semantic edges.")
            return
        
        # L·∫•y danh s√°ch t·ª´ v√† POS tags
        words = []
        pos_tags = []
        word_node_mapping = {}
        
        for node_id in word_nodes:
            word = self.graph.nodes[node_id]['text']
            pos = self.graph.nodes[node_id].get('pos', '')
            words.append(word)
            pos_tags.append(pos)
            word_node_mapping[word] = node_id
        
        print(f"ƒêang l·∫•y embeddings cho {len(words)} t·ª´...")
        
        # L·∫•y embeddings (s·ª≠ d·ª•ng full PhoBERT embeddings - kh√¥ng PCA)
        embeddings = self.get_word_embeddings(words)
        if embeddings is None:
            print("Kh√¥ng th·ªÉ l·∫•y embeddings.")
            return
        
        print(f"ƒê√£ l·∫•y embeddings v·ªõi shape: {embeddings.shape}")
        print("‚úÖ S·ª≠ d·ª•ng full PhoBERT embeddings (768 dim) - KH√îNG √°p d·ª•ng PCA")
        
        # X√¢y d·ª±ng Faiss index (optional)
        if use_faiss:
            print("ƒêang x√¢y d·ª±ng Faiss index v·ªõi full embeddings...")
            dimension = embeddings.shape[1]
            self.faiss_index = faiss.IndexFlatIP(dimension)  # Inner Product (for cosine similarity)
            
            # Normalize vectors for cosine similarity
            embeddings_normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
            self.faiss_index.add(embeddings_normalized.astype(np.float32))
            
            # Create mappings
            self.word_to_index = {word: i for i, word in enumerate(words)}
            self.index_to_word = {i: word for i, word in enumerate(words)}
            print("Faiss index ƒë√£ ƒë∆∞·ª£c x√¢y d·ª±ng.")
        else:
            # Normalize embeddings ƒë·ªÉ t√≠nh cosine similarity nhanh h∆°n
            embeddings_normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
        
        # T√¨m similar words v√† t·∫°o edges
        edges_added = 0
        print(f"ƒêang t√¨m t·ª´ t∆∞∆°ng ƒë·ªìng v·ªõi threshold={self.similarity_threshold}, top_k={self.top_k_similar}...")
        
        for i, word1 in enumerate(words):
            pos1 = pos_tags[i]
            node1 = word_node_mapping[word1]
            
            if use_faiss and self.faiss_index is not None:
                # S·ª≠ d·ª•ng Faiss ƒë·ªÉ t√¨m similar words
                query_vector = embeddings_normalized[i:i+1].astype(np.float32)
                similarities, indices = self.faiss_index.search(query_vector, self.top_k_similar + 1)  # +1 v√¨ s·∫Ω bao g·ªìm ch√≠nh n√≥
                
                for j, (similarity, idx) in enumerate(zip(similarities[0], indices[0])):
                    if idx == i:  # Skip ch√≠nh n√≥
                        continue
                    
                    if similarity < self.similarity_threshold:
                        continue
                    
                    word2 = self.index_to_word[idx]
                    pos2 = pos_tags[idx]
                    node2 = word_node_mapping[word2]
                    
                    # Ch·ªâ k·∫øt n·ªëi t·ª´ c√πng lo·∫°i POS (optional)
                    if pos1 and pos2 and pos1 == pos2:
                        if not self.graph.has_edge(node1, node2):
                            self.graph.add_edge(node1, node2, 
                                              relation="semantic_similar", 
                                              edge_type="semantic",
                                              similarity=float(similarity))
                            edges_added += 1
            else:
                # S·ª≠ d·ª•ng numpy matrix multiplication (nhanh h∆°n sklearn cho cosine similarity)
                for j, word2 in enumerate(words):
                    if i >= j:  # Tr√°nh duplicate v√† self-comparison
                        continue
                    
                    pos2 = pos_tags[j]
                    
                    # Ch·ªâ so s√°nh t·ª´ c√πng lo·∫°i POS
                    if pos1 and pos2 and pos1 != pos2:
                        continue
                    
                    # T√≠nh cosine similarity v·ªõi normalized vectors (nhanh h∆°n)
                    similarity = np.dot(embeddings_normalized[i], embeddings_normalized[j])
                    
                    if similarity >= self.similarity_threshold:
                        node2 = word_node_mapping[word2]
                        if not self.graph.has_edge(node1, node2):
                            self.graph.add_edge(node1, node2, 
                                              relation="semantic_similar", 
                                              edge_type="semantic",
                                              similarity=float(similarity))
                            edges_added += 1
        
        print(f"ƒê√£ th√™m {edges_added} semantic similarity edges.")
        return edges_added
    
    def get_semantic_statistics(self):
        """Th·ªëng k√™ v·ªÅ semantic edges"""
        semantic_edges = [
            (u, v, data) for u, v, data in self.graph.edges(data=True) 
            if data.get('edge_type') == 'semantic'
        ]
        
        if not semantic_edges:
            return {
                "total_semantic_edges": 0,
                "average_similarity": 0.0,
                "similarity_distribution": {}
            }
        
        similarities = [data.get('similarity', 0.0) for u, v, data in semantic_edges]
        
        return {
            "total_semantic_edges": len(semantic_edges),
            "average_similarity": np.mean(similarities),
            "max_similarity": np.max(similarities),
            "min_similarity": np.min(similarities),
            "similarity_distribution": {
                "0.85-0.90": len([s for s in similarities if 0.85 <= s < 0.90]),
                "0.90-0.95": len([s for s in similarities if 0.90 <= s < 0.95]),
                "0.95-1.00": len([s for s in similarities if 0.95 <= s <= 1.00])
            }
        }
    
    def beam_search_paths(self, beam_width=10, max_depth=6, max_paths=20):
        """
        T√¨m ƒë∆∞·ªùng ƒëi t·ª´ claim ƒë·∫øn sentence nodes b·∫±ng Beam Search
        
        Args:
            beam_width (int): ƒê·ªô r·ªông beam search
            max_depth (int): ƒê·ªô s√¢u t·ªëi ƒëa c·ªßa path
            max_paths (int): S·ªë l∆∞·ª£ng paths t·ªëi ƒëa tr·∫£ v·ªÅ
            
        Returns:
            List[Path]: Danh s√°ch paths t·ªët nh·∫•t
        """
        if not self.claim_node:
            print("‚ö†Ô∏è Kh√¥ng c√≥ claim node ƒë·ªÉ th·ª±c hi·ªán beam search")
            return []
            
        # T·∫°o BeamSearchPathFinder
        path_finder = BeamSearchPathFinder(
            text_graph=self,
            beam_width=beam_width,
            max_depth=max_depth
        )
        
        # T√¨m paths
        paths = path_finder.find_best_paths(max_paths=max_paths)
        
        return paths
    
    def export_beam_search_results(self, paths, output_dir="output", file_prefix="beam_search"):
        """
        Export k·∫øt qu·∫£ beam search ra files
        
        Args:
            paths: Danh s√°ch paths t·ª´ beam search
            output_dir (str): Th∆∞ m·ª•c output
            file_prefix (str): Prefix cho t√™n file
            
        Returns:
            tuple: (json_file_path, summary_file_path)
        """
        if not paths:
            print("‚ö†Ô∏è Kh√¥ng c√≥ paths ƒë·ªÉ export")
            return None, None
            
        # T·∫°o BeamSearchPathFinder ƒë·ªÉ export
        path_finder = BeamSearchPathFinder(self)
        
        # Export JSON v√† summary v·ªõi absolute paths
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Ensure we use the correct directory
        current_dir = os.getcwd()
        if current_dir.endswith('vncorenlp'):
            # If we're in vncorenlp directory, go back to parent
            current_dir = os.path.dirname(current_dir)
        
        json_file = os.path.join(current_dir, output_dir, f"{file_prefix}_{timestamp}.json")
        summary_file = os.path.join(current_dir, output_dir, f"{file_prefix}_summary_{timestamp}.txt")
        
        json_path = path_finder.export_paths_to_file(paths, json_file)
        summary_path = path_finder.export_paths_summary(paths, summary_file)
        
        return json_path, summary_path
    
    def analyze_paths_quality(self, paths):
        """
        Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng c·ªßa c√°c paths t√¨m ƒë∆∞·ª£c
        
        Args:
            paths: Danh s√°ch paths
            
        Returns:
            dict: Th·ªëng k√™ v·ªÅ paths
        """
        if not paths:
            return {
                'total_paths': 0,
                'avg_score': 0,
                'avg_length': 0,
                'paths_to_sentences': 0,
                'paths_through_entities': 0
            }
            
        total_paths = len(paths)
        scores = [p.score for p in paths]
        lengths = [len(p.nodes) for p in paths]
        
        sentences_reached = sum(1 for p in paths if any(
            node.startswith('sentence') for node in p.nodes
        ))
        
        entities_visited = sum(1 for p in paths if p.entities_visited)
        
        return {
            'total_paths': total_paths,
            'avg_score': sum(scores) / total_paths if scores else 0,
            'max_score': max(scores) if scores else 0,
            'min_score': min(scores) if scores else 0,
            'avg_length': sum(lengths) / total_paths if lengths else 0,
            'max_length': max(lengths) if lengths else 0,
            'min_length': min(lengths) if lengths else 0,
            'paths_to_sentences': sentences_reached,
            'paths_through_entities': entities_visited,
            'sentence_reach_rate': sentences_reached / total_paths if total_paths > 0 else 0,
            'entity_visit_rate': entities_visited / total_paths if total_paths > 0 else 0
        }
    
    def multi_level_beam_search_paths(
        self,
        max_levels: int = 3,
        beam_width_per_level: int = 3,
        max_depth: int = 30,
        allow_skip_edge: bool = False,        # üÜï b·∫≠t/t·∫Øt 2-hops
        min_new_sentences: int = 0,            # ƒë√£ c√≥ t·ª´ l·∫ßn tr∆∞·ªõc
        advanced_data_filter=None,
        claim_text="",
        entities=None,
        filter_top_k: int = 2
    ) -> Dict[int, List]:
        """
        Multi-level beam search wrapper cho TextGraph
        
        Args:
            max_levels: S·ªë levels t·ªëi ƒëa
            beam_width_per_level: S·ªë sentences m·ªói level
            max_depth: ƒê·ªô s√¢u t·ªëi ƒëa cho beam search
            
        Returns:
            Dict[level, List[Path]]: Results theo t·ª´ng level
        """
        if not self.claim_node:
            print("‚ö†Ô∏è Kh√¥ng c√≥ claim node ƒë·ªÉ th·ª±c hi·ªán multi-level beam search")
            return {}
            
        # T·∫°o BeamSearchPathFinder v·ªõi custom max_depth
        path_finder = BeamSearchPathFinder(
            text_graph=self,
            beam_width=25,
            max_depth=max_depth,
            allow_skip_edge=allow_skip_edge    # üÜï chuy·ªÉn tham s·ªë
        )
        
        # Ch·∫°y multi-level search
        multi_results = path_finder.multi_level_beam_search(
            max_levels=max_levels,
            beam_width_per_level=beam_width_per_level,
            min_new_sentences=min_new_sentences,
            advanced_data_filter=advanced_data_filter,
            claim_text=claim_text,
            entities=entities,
            filter_top_k=filter_top_k
        )
        
        return multi_results 
        
    def multi_level_beam_search_paths_from_start_nodes(
        self,
        start_nodes: List[str],
        max_levels: int = 3,
        beam_width_per_level: int = 3,
        max_depth: int = 30,
        allow_skip_edge: bool = False,
        min_new_sentences: int = 0,
        advanced_data_filter=None,
        claim_text="",
        entities=None,
        filter_top_k: int = 2
    ) -> Dict[int, List]:
        """
        Multi-level beam search t·ª´ c√°c start nodes c·ª• th·ªÉ (thay v√¨ t·ª´ claim node)
        
        Args:
            start_nodes: List c√°c node IDs ƒë·ªÉ b·∫Øt ƒë·∫ßu search
            max_levels: S·ªë levels t·ªëi ƒëa
            beam_width_per_level: S·ªë sentences m·ªói level
            max_depth: ƒê·ªô s√¢u t·ªëi ƒëa cho beam search
            
        Returns:
            Dict[level, List[Path]]: Results theo t·ª´ng level
        """
        if not start_nodes:
            print("‚ö†Ô∏è Kh√¥ng c√≥ start nodes ƒë·ªÉ th·ª±c hi·ªán multi-level beam search")
            return {}
            
        # T·∫°o BeamSearchPathFinder v·ªõi custom max_depth
        path_finder = BeamSearchPathFinder(
            text_graph=self,
            beam_width=25,
            max_depth=max_depth,
            allow_skip_edge=allow_skip_edge
        )
        
        # Ch·∫°y multi-level search t·ª´ start nodes
        multi_results = path_finder.multi_level_beam_search_from_start_nodes(
            start_nodes=start_nodes,
            max_levels=max_levels,
            beam_width_per_level=beam_width_per_level,
            min_new_sentences=min_new_sentences,
            advanced_data_filter=advanced_data_filter,
            claim_text=claim_text,
            entities=entities,
            filter_top_k=filter_top_k
        )
        
        return multi_results 

    def extract_claim_keywords_with_openai(self, claim_text):
        """Tr√≠ch xu·∫•t keywords quan tr·ªçng t·ª´ claim ƒë·ªÉ t·∫°o th√™m entities"""
        if not self.openai_client:
            print("OpenAI client ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o. Kh√¥ng th·ªÉ tr√≠ch xu·∫•t claim keywords.")
            return []
        
        try:
            prompt = f"""
B·∫°n l√† chuy√™n gia ph√¢n t√≠ch ng√¥n ng·ªØ cho h·ªá th·ªëng fact-checking. H√£y tr√≠ch xu·∫•t T·∫§T C·∫¢ c√°c t·ª´ kh√≥a quan tr·ªçng t·ª´ c√¢u claim d∆∞·ªõi ƒë√¢y.

M√î H√åNH TR√çCH XU·∫§T:
1. **CH·ª¶ TH·ªÇ CH√çNH** (ai/c√°i g√¨): t√™n ng∆∞·ªùi, t·ªï ch·ª©c, s·∫£n ph·∫©m, lo√†i v·∫≠t, ƒë·ªãa danh
2. **H√ÄNH ƒê·ªòNG/ƒê·ªòNG T·ª™** quan tr·ªçng: s·ª≠ d·ª•ng, ph√°t tri·ªÉn, t·∫°o ra, gi·∫£i m√£, hi·ªÉu, giao ti·∫øp
3. **ƒê·ªêI T∆Ø·ª¢NG/KH√ÅI NI·ªÜM** quan tr·ªçng: c√¥ng ngh·ªá, khoa h·ªçc, nghi√™n c·ª©u, ph∆∞∆°ng ph√°p
4. **T√çNH CH·∫§T/TR·∫†NG TH√ÅI**: m·ªõi, hi·ªán ƒë·∫°i, ti√™n ti·∫øn, th√†nh c√¥ng

NGUY√äN T·∫ÆC TR√çCH XU·∫§T:
- L·∫•y CH√çNH X√ÅC t·ª´/c·ª•m t·ª´ c√≥ trong claim
- L·∫•y c·∫£ t·ª´ ƒë∆°n l·∫ª V√Ä c·ª•m t·ª´ c√≥ √Ω nghƒ©a
- T·∫≠p trung v√†o t·ª´ kh√≥a c√≥ th·ªÉ fact-check ƒë∆∞·ª£c
- Kh√¥ng th√™m t·ª´ kh√¥ng c√≥ trong claim

V√ç D·ª§:
INPUT: "T·∫≠n d·ª•ng c√¥ng ngh·ªá m·ªõi ƒë·ªÉ hi·ªÉu giao ti·∫øp c·ªßa ƒë·ªông v·∫≠t"
OUTPUT: ["t·∫≠n d·ª•ng", "c√¥ng ngh·ªá", "c√¥ng ngh·ªá m·ªõi", "hi·ªÉu", "giao ti·∫øp", "ƒë·ªông v·∫≠t", "giao ti·∫øp c·ªßa ƒë·ªông v·∫≠t"]

INPUT: "Thay v√¨ c·ªë g·∫Øng d·∫°y chim n√≥i ti·∫øng Anh, c√°c nh√† nghi√™n c·ª©u ƒëang gi·∫£i m√£ nh·ªØng g√¨ ch√∫ng n√≥i v·ªõi nhau b·∫±ng ti·∫øng chim"
OUTPUT: ["thay v√¨", "c·ªë g·∫Øng", "d·∫°y", "chim", "n√≥i", "ti·∫øng Anh", "nh√† nghi√™n c·ª©u", "gi·∫£i m√£", "ti·∫øng chim", "giao ti·∫øp", "d·∫°y chim n√≥i ti·∫øng Anh", "nh√† nghi√™n c·ª©u gi·∫£i m√£", "chim n√≥i"]

INPUT: "Nh√† khoa h·ªçc Vi·ªát Nam ph√°t tri·ªÉn AI ƒë·ªÉ d·ª± b√°o th·ªùi ti·∫øt"
OUTPUT: ["nh√† khoa h·ªçc", "Vi·ªát Nam", "nh√† khoa h·ªçc Vi·ªát Nam", "ph√°t tri·ªÉn", "AI", "d·ª± b√°o", "th·ªùi ti·∫øt", "d·ª± b√°o th·ªùi ti·∫øt"]

INPUT: "Apple s·ª≠ d·ª•ng chip M1 m·ªõi trong MacBook Pro 2021"
OUTPUT: ["Apple", "s·ª≠ d·ª•ng", "chip", "M1", "chip M1", "m·ªõi", "MacBook Pro", "2021", "MacBook Pro 2021"]

Tr·∫£ v·ªÅ JSON array v·ªõi t·∫•t c·∫£ keywords quan tr·ªçng: ["keyword1", "keyword2", ...]

CLAIM: {claim_text}
"""

            model = getattr(self, 'openai_model', 'gpt-4o-mini')
            temperature = getattr(self, 'openai_temperature', 0.0)
            max_tokens = getattr(self, 'openai_max_tokens', 500)

            response = self.openai_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                max_tokens=max_tokens
            )
            
            response_text = response.choices[0].message.content.strip()
            
            # Strip markdown code blocks if present
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.startswith('```'):
                response_text = response_text[3:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            try:
                keywords = json.loads(response_text)
                if isinstance(keywords, list):
                    keywords = list(set([kw.strip() for kw in keywords if kw.strip()]))
                    return keywords
                else:
                    return []
            except json.JSONDecodeError:
                return []
                
        except Exception as e:
            return []

    def extract_enhanced_entities_with_openai(self, context_text, claim_text):
        """Enhanced entity extraction: 2 separate prompting approaches then combine"""
        
        print(f"üéØ DEBUG CLAIM TEXT: {claim_text}")
        
        # üîß PREPROCESS: Clean up VnCoreNLP format (remove underscores)
        if context_text:
            context_clean = context_text.replace("_", " ").strip()
        else:
            context_clean = ""
            
        claim_clean = claim_text.replace("_", " ").strip() if claim_text else ""
        print(f"üéØ DEBUG CLAIM CLEAN: {claim_clean}")
        
        # üéØ PROMPTING 1: Extract entities from context + claim (original approach)
        context_claim_entities = []
        if context_clean and len(context_clean.strip()) > 10:
            try:
                # Use improved context entity extraction 
                context_entities = self.extract_context_entities_improved(context_clean)
                # Combine v·ªõi claim entities ƒë∆∞·ª£c extract ri√™ng
                context_claim_entities = context_entities
                # Debug context entities extracted
            except Exception as e:
                print(f"‚ö†Ô∏è Context entity extraction failed: {e}")
                pass
        
        # üéØ PROMPTING 2: Extract detailed keywords from claim only
        claim_keywords = []
        if claim_clean:
            try:
                claim_keywords = self.extract_claim_keywords_with_openai(claim_clean)
                # Debug claim keywords extracted
            except Exception as e:
                pass
        
        # üîó Step 3: Combine two separate arrays then deduplicate
        # Combine v√† deduplicate
        all_entities = list(set(context_claim_entities + claim_keywords))
        
        # ‚úÖ M·ªöI: L∆∞u claim entities ƒë·ªÉ scoring
        self.claim_entities = set(claim_keywords)  # L∆∞u claim keywords l√†m claim entities
        # Claim entities saved for scoring boost
        
        # üÜï Store entities globally for multi-hop reuse
        if not hasattr(self, 'global_entities'):
            self.global_entities = []
        
        # Add new entities to global pool
        new_entities = [e for e in all_entities if e not in self.global_entities]
        self.global_entities.extend(new_entities)
        
        return all_entities

    def extract_context_entities_improved(self, context_text):
        """Extract entities t·ª´ context v·ªõi prompt c·∫£i thi·ªán v√† chi ti·∫øt h∆°n"""
        if not self.openai_client:
            return []
        
        try:
            prompt = f"""
H√£y tr√≠ch xu·∫•t T·∫§T C·∫¢ th·ª±c th·ªÉ quan tr·ªçng t·ª´ vƒÉn b·∫£n ti·∫øng Vi·ªát sau ƒë√¢y.

QUY T·∫ÆC TR√çCH XU·∫§T:
1. Ch·ªâ l·∫•y t·ª´/c·ª•m t·ª´ C√ì TRONG vƒÉn b·∫£n
2. Gi·ªØ nguy√™n ch√≠nh t·∫£ nh∆∞ trong vƒÉn b·∫£n
3. L·∫•y c·∫£ t·ª´ ƒë∆°n l·∫ª V√Ä c·ª•m t·ª´ c√≥ √Ω nghƒ©a

LO·∫†I TH·ª∞C TH·ªÇ C·∫¶N L·∫§Y:
‚úÖ T√™n ng∆∞·ªùi: "Nguy·ªÖn VƒÉn A", "John Smith", "Einstein"
‚úÖ T√™n t·ªï ch·ª©c: "SAWACO", "Microsoft", "ƒê·∫°i h·ªçc Stanford", "NASA"
‚úÖ ƒê·ªãa danh: "TP.HCM", "Vi·ªát Nam", "London", "Qu·∫≠n 1"
‚úÖ S·∫£n ph·∫©m/C√¥ng ngh·ªá: "iPhone", "AI", "machine learning", "ChatGPT"
‚úÖ Ng√†y th√°ng/S·ªë: "25-3", "2023", "85%", "15 tri·ªáu ƒë·ªìng"
‚úÖ Kh√°i ni·ªám khoa h·ªçc: "nghi√™n c·ª©u", "ph√°t tri·ªÉn", "c√¥ng ngh·ªá", "khoa h·ªçc"
‚úÖ ƒê·ªông v·∫≠t/Sinh v·∫≠t: "voi", "chim", "voi ch√¢u Phi", "ƒë·ªông v·∫≠t"
‚úÖ T·∫°p ch√≠/·∫§n ph·∫©m: "Nature", "Science", "t·∫°p ch√≠"

V√ç D·ª§:
INPUT: "C√°c nh√† khoa h·ªçc t·∫°i ƒê·∫°i h·ªçc Stanford ƒë√£ ph√°t tri·ªÉn AI ƒë·ªÉ nghi√™n c·ª©u voi ch√¢u Phi"
OUTPUT: ["nh√† khoa h·ªçc", "ƒê·∫°i h·ªçc Stanford", "ph√°t tri·ªÉn", "AI", "nghi√™n c·ª©u", "voi ch√¢u Phi", "voi", "ch√¢u Phi"]

QUAN TR·ªåNG: Tr·∫£ v·ªÅ JSON array, kh√¥ng gi·∫£i th√≠ch th√™m.

VƒÉn b·∫£n:
{context_text}
"""

            response = self.openai_client.chat.completions.create(
                model=getattr(self, 'openai_model', 'gpt-4o-mini'),
                messages=[{"role": "user", "content": prompt}],
                temperature=0,
                max_tokens=1000  # TƒÉng token limit
            )
            
            response_text = response.choices[0].message.content.strip()
            print(f"üîç OpenAI raw response: {response_text[:200]}...")
            
            # Parse JSON
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.startswith('```'):
                response_text = response_text[3:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            entities = json.loads(response_text)
            if isinstance(entities, list):
                entities = [e.strip() for e in entities if e.strip()]
                print(f"üìÑ Improved context extraction: {len(entities)} entities")
                return entities
            else:
                print(f"‚ùå Response not a list: {response_text}")
                return []
            
        except json.JSONDecodeError as e:
            print(f"‚ùå JSON parse error: {e}")
            print(f"‚ùå Raw response: {response_text}")
            return []
        except Exception as e:
            print(f"‚ùå Improved context extraction error: {e}")
            return []

    def extract_context_entities_simple(self, context_text):
        """Extract entities t·ª´ context v·ªõi prompt ƒë∆°n gi·∫£n h∆°n"""
        if not self.openai_client:
            return []
        
        try:
            prompt = f"""
Tr√≠ch xu·∫•t t·∫•t c·∫£ th·ª±c th·ªÉ quan tr·ªçng t·ª´ vƒÉn b·∫£n sau. Ch·ªâ l·∫•y nh·ªØng t·ª´/c·ª•m t·ª´ c√≥ trong vƒÉn b·∫£n.

LO·∫†I TH·ª∞C TH·ªÇ C·∫¶N L·∫§Y:
- T√™n ng∆∞·ªùi: "Nguy·ªÖn VƒÉn A", "John Smith"
- T√™n t·ªï ch·ª©c/c√¥ng ty: "SAWACO", "Microsoft", "ƒê·∫°i h·ªçc B√°ch Khoa"
- ƒê·ªãa danh: "TP.HCM", "Vi·ªát Nam", "Qu·∫≠n 1"
- S·∫£n ph·∫©m/c√¥ng ngh·ªá: "iPhone", "AI", "ChatGPT"
- Ng√†y th√°ng: "25-3", "2023", "th√°ng 6"
- S·ªë l∆∞·ª£ng c√≥ √Ω nghƒ©a: "15 tri·ªáu ƒë·ªìng", "69 t·∫•n", "100 ng∆∞·ªùi"
- Kh√°i ni·ªám quan tr·ªçng: "nghi√™n c·ª©u", "khoa h·ªçc", "ph√°t tri·ªÉn"

Tr·∫£ v·ªÅ JSON array: ["entity1", "entity2", ...]

VƒÉn b·∫£n:
{context_text}
"""

            response = self.openai_client.chat.completions.create(
                model=getattr(self, 'openai_model', 'gpt-4o-mini'),
                messages=[{"role": "user", "content": prompt}],
                temperature=0,
                max_tokens=800
            )
            
            response_text = response.choices[0].message.content.strip()
            
            # Parse JSON
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.startswith('```'):
                response_text = response_text[3:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            entities = json.loads(response_text)
            if isinstance(entities, list):
                entities = [e.strip() for e in entities if e.strip()]
                print(f"üìÑ Simple context extraction: {len(entities)} entities")
                return entities
            return []
            
        except Exception as e:
            print(f"‚ùå Simple context extraction error: {e}")
            return []

    def get_global_entities(self):
        """L·∫•y danh s√°ch entities ƒë√£ ƒë∆∞·ª£c thu th·∫≠p qua c√°c l·∫ßn extraction"""
        return getattr(self, 'global_entities', [])

    def add_to_global_entities(self, new_entities):
        """Th√™m entities m·ªõi v√†o global pool"""
        if not hasattr(self, 'global_entities'):
            self.global_entities = []
        
        added = 0
        for entity in new_entities:
            if entity not in self.global_entities:
                self.global_entities.append(entity)
                added += 1
        
        print(f"üåç Added {added} new entities to global pool (total: {len(self.global_entities)})")
        return added

    def get_claim_entities(self):
        """L·∫•y danh s√°ch claim entities ƒë·ªÉ boost scoring"""
        return getattr(self, 'claim_entities', set())
    
    def get_sentences_connected_to_claim_entities(self):
        """L·∫•y t·∫•t c·∫£ sentences ƒë∆∞·ª£c n·ªëi tr·ª±c ti·∫øp v·ªõi claim entities"""
        if not hasattr(self, 'claim_entities') or not self.claim_entities:
            return []
        
        connected_sentences = set()
        
        # Duy·ªát qua t·∫•t c·∫£ nodes trong graph ƒë·ªÉ t√¨m entity nodes c√≥ text matching claim entities
        for node_id, node_data in self.graph.nodes(data=True):
            if node_data.get('type') == 'entity':
                entity_text = node_data.get('text', '')
                
                # Ki·ªÉm tra xem entity text c√≥ trong claim entities kh√¥ng
                if entity_text in self.claim_entities:
                    # L·∫•y t·∫•t c·∫£ neighbors c·ªßa entity node
                    for neighbor in self.graph.neighbors(node_id):
                        # N·∫øu neighbor l√† sentence node
                        if neighbor.startswith('sentence_'):
                            sentence_text = self.graph.nodes[neighbor]['text']
                            connected_sentences.add((neighbor, sentence_text))
        
        # Convert th√†nh list v√† sort theo sentence index
        result = list(connected_sentences)
        result.sort(key=lambda x: int(x[0].split('_')[1]))  # Sort by sentence index
        
        print(f"üéØ Found {len(result)} sentences directly connected to claim entities")
        return result
    
    def get_sentences_connected_to_claim_by_similarity(self):
        """L·∫•y sentences ƒë∆∞·ª£c n·ªëi tr·ª±c ti·∫øp v·ªõi claim b·∫±ng text similarity"""
        if not self.claim_node:
            return []
        
        connected_sentences = []
        
        # L·∫•y t·∫•t c·∫£ neighbors c·ªßa claim node
        for neighbor in self.graph.neighbors(self.claim_node):
            if neighbor.startswith('sentence_'):
                # Ki·ªÉm tra xem c√≥ ph·∫£i l√† text similarity connection kh√¥ng
                edge_data = self.graph.get_edge_data(neighbor, self.claim_node)
                if edge_data and edge_data.get('relation') == 'text_similar':
                    sentence_text = self.graph.nodes[neighbor]['text']
                    similarity = edge_data.get('similarity', 0.0)
                    connected_sentences.append((neighbor, sentence_text, similarity))
        
        # Sort theo similarity score gi·∫£m d·∫ßn
        connected_sentences.sort(key=lambda x: x[2], reverse=True)
        
        print(f"üîó Found {len(connected_sentences)} sentences connected to claim by similarity")
        return connected_sentences
    
    def get_high_confidence_evidence_sentences(self):
        """L·∫•y sentences c√≥ ƒë·ªô tin c·∫≠y cao: n·ªëi v·ªõi claim entities + similarity v·ªõi claim"""
        entity_sentences = self.get_sentences_connected_to_claim_entities()
        similarity_sentences = self.get_sentences_connected_to_claim_by_similarity()
        
        # Combine v√† remove duplicates
        all_sentences = {}
        
        # Add entity-connected sentences v·ªõi high priority
        for sent_id, sent_text in entity_sentences:
            all_sentences[sent_id] = {
                'text': sent_text,
                'connected_to_entities': True,
                'similarity_score': 0.0,
                'confidence': 'high'  # Entity connection = high confidence
            }
        
        # Add similarity-connected sentences
        for sent_id, sent_text, similarity in similarity_sentences:
            if sent_id not in all_sentences:
                all_sentences[sent_id] = {
                    'text': sent_text,
                    'connected_to_entities': False,
                    'similarity_score': similarity,
                    'confidence': 'medium' if similarity >= 0.25 else 'low'
                }
            else:
                # Update existing v·ªõi similarity score
                all_sentences[sent_id]['similarity_score'] = similarity
                all_sentences[sent_id]['confidence'] = 'very_high'  # Both entity + similarity
        
        # Convert to sorted list
        result = []
        for sent_id, data in all_sentences.items():
            result.append({
                'sentence_id': sent_id,
                'text': data['text'],
                'connected_to_entities': data['connected_to_entities'],
                'similarity_score': data['similarity_score'],
                'confidence': data['confidence']
            })
        
        # Sort by confidence level then similarity
        confidence_order = {'very_high': 4, 'high': 3, 'medium': 2, 'low': 1}
        result.sort(key=lambda x: (confidence_order[x['confidence']], x['similarity_score']), reverse=True)
        
        print(f"‚ú® Found {len(result)} high-confidence evidence sentences")
        return result 