#!/usr/bin/env python3
"""
ğŸš€ BEAM GRAPH FILTER PIPELINE
=============================

Pipeline má»›i Ä‘á»ƒ:
1. Tiá»n xá»­ lÃ½ context (xoÃ¡ xuá»‘ng dÃ²ng, chuáº©n hoÃ¡ khoáº£ng tráº¯ng, bá» dáº¥u cÃ¡ch trÆ°á»›c dáº¥u cÃ¢u).
2. Cáº¯t cÃ¢u báº±ng regex.
3. DÃ¹ng VnCoreNLP Ä‘á»ƒ tÃ¡ch tá»«, POS, dependency.
4. XÃ¢y TextGraph, cháº¡y Beam Search Ä‘á»ƒ láº¥y táº­p cÃ¢u liÃªn quan.
5. Lá»c láº¡i táº­p cÃ¢u báº±ng AdvancedDataFilter(use_sbert=False, use_contradiction_detection=False, use_nli=False).

Usage:
    python beam_graph_filter_pipeline.py --input raw_test.json --output_dir advanced_filtering_output \
           --min_relevance 0.15 --beam_width 20 --max_depth 40

Author: AI Assistant & NguyenNha
Date: 2025-07-12
"""

import os
import sys
import json
import re
import argparse
from typing import List, Dict
from datetime import datetime
import contextlib
import io

# Báº£o Ä‘áº£m import Ä‘Æ°á»£c py_vncorenlp (Ä‘Æ°á»ng dáº«n chá»©a VnCoreNLP-1.2.jar vÃ  models)
VNCORENLP_DIR = os.path.join(os.path.dirname(__file__), "vncorenlp")
sys.path.append(VNCORENLP_DIR)
import py_vncorenlp  # type: ignore

from mint.text_graph import TextGraph
from advanced_data_filtering import AdvancedDataFilter

###############################################################################
# ğŸ› ï¸  TIá»†N ÃCH TIá»€N Xá»¬ LÃ
###############################################################################

def clean_text(text: str) -> str:
    """Loáº¡i bá» xuá»‘ng dÃ²ng, chuáº©n hoÃ¡ khoáº£ng tráº¯ng vÃ  xoÃ¡ khoáº£ng tráº¯ng trÆ°á»›c dáº¥u cÃ¢u"""
    if not text:
        return ""
    text = text.replace("\n", " ")
    text = re.sub(r"\s+", " ", text)
    # Bá» khoáº£ng tráº¯ng trÆ°á»›c dáº¥u cÃ¢u (, . ; : ! ? )
    text = re.sub(r"\s+([,.;!?])", r"\1", text)
    # Bá» khoáº£ng tráº¯ng sau "("
    text = re.sub(r"\(\s+", "(", text)
    # Bá» khoáº£ng tráº¯ng trÆ°á»›c ")"
    text = re.sub(r"\s+\)", ")", text)
    return text.strip()


def split_sentences(text: str) -> List[str]:
    """Cáº¯t cÃ¢u Ä‘Æ¡n giáº£n báº±ng regex: sau . ! ? vÃ  khoáº£ng tráº¯ng"""
    if not text:
        return []
    raw = re.split(r"(?<=[.!?])\s+", text)
    return [s.strip() for s in raw if s.strip()]

###############################################################################
# ğŸ› ï¸  EXTRACT SENTENCES Tá»ª BEAM SEARCH PATHS
###############################################################################

def extract_sentences_from_paths(paths, text_graph: TextGraph, top_n: int | None = 30) -> List[Dict]:
    """TrÃ­ch xuáº¥t sentences duy nháº¥t tá»« cÃ¡c BeamSearch Path, kÃ¨m score cao nháº¥t"""
    if not paths:
        return []

    sentence_best_score = {}
    for path_obj in paths:
        path_score = getattr(path_obj, "score", 0.0)
        path_nodes = getattr(path_obj, "nodes", [])
        for node_id in path_nodes:
            if node_id.startswith("sentence") and node_id in text_graph.graph.nodes:
                sent_text = text_graph.graph.nodes[node_id].get("text", "")
                if not sent_text:
                    continue
                prev = sentence_best_score.get(sent_text)
                if prev is None or path_score > prev:
                    sentence_best_score[sent_text] = path_score

    # Sáº¯p xáº¿p giáº£m dáº§n theo score
    sorted_sentences = sorted(sentence_best_score.items(), key=lambda x: x[1], reverse=True)
    if top_n is None:
        return [{"sentence": s, "score": sc} for s, sc in sorted_sentences]
    return [{"sentence": s, "score": sc} for s, sc in sorted_sentences[:top_n]]

###############################################################################
# ğŸš€ Xá»¬ LÃ Má»˜T SAMPLE
###############################################################################

def process_sample(sample: Dict, model, filter_sys: AdvancedDataFilter, min_relevance: float,
                   beam_width: int, max_depth: int, max_paths: int,
                   max_final_sentences: int = 30, beam_sentences: int = 50):
    """Process má»™t sample, tráº£ vá» (raw_count, beam_count, final_count)"""
    context_raw = sample.get("context", "")
    claim = sample.get("claim", "")

    # 1ï¸âƒ£ Tiá»n xá»­ lÃ½ vÃ  cáº¯t cÃ¢u (cho debug / fallback)
    context_clean = clean_text(context_raw)
    raw_sentences = split_sentences(context_clean)

    # 2ï¸âƒ£ VnCoreNLP annotate
    context_tokens = model.annotate_text(context_clean)
    claim_tokens = model.annotate_text(claim)

    # 3ï¸âƒ£ Build TextGraph
    tg = TextGraph()
    tg.build_from_vncorenlp_output(context_tokens, claim, claim_tokens)

    # 4ï¸âƒ£ Beam Search láº¥y path -> sentences
    paths = tg.beam_search_paths(beam_width=beam_width, max_depth=max_depth, max_paths=max_paths)
    candidate_sentences = extract_sentences_from_paths(paths, tg, top_n=beam_sentences)

    # Fallback náº¿u beam khÃ´ng ra cÃ¢u nÃ o
    if not candidate_sentences:
        candidate_sentences = [{"sentence": s} for s in raw_sentences]

    # 5ï¸âƒ£ AdvancedDataFilter (luÃ´n báº­t â€“ log bá»‹ áº©n Ä‘á»ƒ gá»n console)
    silent_buf = io.StringIO()
    with contextlib.redirect_stdout(silent_buf):
        results = filter_sys.multi_stage_filtering_pipeline(
            sentences=candidate_sentences,
            claim_text=claim,
            min_relevance_score=min_relevance,
            max_final_sentences=max_final_sentences
        )
    final_sentences = results["filtered_sentences"]

    sample["filtered_evidence"] = [d["sentence"] for d in final_sentences]

    # --- Chuáº©n hoÃ¡ káº¿t quáº£ giá»‘ng process_multi_hop_multi_beam_search ---
    simple_result = {
        **{k: sample.get(k) for k in ("context", "claim", "evidence", "label") if k in sample},
        "multi_level_evidence": [d["sentence"] for d in final_sentences]
    }
    detailed_result = {
        **{k: sample.get(k) for k in ("context", "claim", "evidence", "label") if k in sample},
        "multi_level_evidence": final_sentences,
        "statistics": {
            "beam": {
                "total_paths": len(paths),
                "unique_sentences": len(candidate_sentences)
            }
        }
    }

    return simple_result, detailed_result, len(raw_sentences), len(candidate_sentences), len(final_sentences)

###############################################################################
# ğŸ MAIN
###############################################################################

def main():
    parser = argparse.ArgumentParser(description="Beam Graph + Advanced Filter pipeline")
    parser.add_argument("--input", type=str, default="raw_test.json", help="File JSON input")
    parser.add_argument("--output_dir", type=str, default="beam_filter_output", help="ThÆ° má»¥c lÆ°u output")
    parser.add_argument("--min_relevance", type=float, default=0.15, help="NgÆ°á»¡ng relevance tá»‘i thiá»ƒu")
    # GiÃ¡ trá»‹ máº·c Ä‘á»‹nh má»›i Ä‘á»ƒ â€œvÃ©tâ€ nhiá»u cÃ¢u hÆ¡n
    parser.add_argument("--beam_width", type=int, default=40,
                         help="Beam width (sá»‘ path giá»¯ má»—i bÆ°á»›c)")
    parser.add_argument("--max_depth", type=int, default=120,
                         help="Äá»™ sÃ¢u tá»‘i Ä‘a cá»§a beam search")
    parser.add_argument("--max_paths", type=int, default=200,
                         help="Sá»‘ paths tá»‘i Ä‘a tráº£ vá»")
    parser.add_argument("--max_final_sentences", type=int, default=30, help="Sá»‘ cÃ¢u cuá»‘i cÃ¹ng giá»¯ láº¡i")
    parser.add_argument("--max_samples", type=int, default=None, help="Giá»›i háº¡n sá»‘ sample xá»­ lÃ½")
    parser.add_argument("--beam_sentences", type=int, default=50,
                    help="Sá»‘ cÃ¢u tá»‘i Ä‘a láº¥y tá»« Beam Search trÆ°á»›c khi lá»c")
    args = parser.parse_args()

    # ğŸ‘‰ luÃ´n dÃ¹ng Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i cho output_dir
    args.output_dir = os.path.abspath(args.output_dir)
    os.makedirs(args.output_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = os.path.join(
        args.output_dir,
        f"{os.path.splitext(os.path.basename(args.input))[0]}_beam_filtered_{args.min_relevance}_{timestamp}.json"
    )

    # ğŸ“ Äá»‹nh nghÄ©a Ä‘Æ°á»ng dáº«n file output (ghi má»™t máº£ng JSON duy nháº¥t sau khi cháº¡y xong)
    simple_file   = output_file.replace(".json", "_simple.json")
    detailed_file = output_file.replace(".json", "_detailed.json")
    stats_file    = output_file.replace(".json", "_stats.json")

    # ğŸ” báº£o Ä‘áº£m thÆ° má»¥c Ä‘Ã­ch tá»“n táº¡i vÃ  reset file cÅ©
    for fp in (simple_file, detailed_file, stats_file):
        os.makedirs(os.path.dirname(fp), exist_ok=True)
    for fp in (simple_file, detailed_file):
        if os.path.exists(fp):
            os.remove(fp)

    # Load samples
    with open(args.input, "r", encoding="utf-8") as f:
        samples = json.load(f)

    # Ãp dá»¥ng max_samples náº¿u cÃ³
    if args.max_samples is not None and args.max_samples < len(samples):
        samples = samples[:args.max_samples]
        print(f"âš ï¸  Chá»‰ xá»­ lÃ½ {len(samples)} sample Ä‘áº§u tiÃªn theo --max_samples={args.max_samples}")

    total_to_process = len(samples)
    print(f"Processing {total_to_process} samples from {args.input} with min_relevance_score={args.min_relevance} ...")

    # Setup VnCoreNLP model
    print("ğŸ”§ Loading VnCoreNLP model ...")
    model = py_vncorenlp.VnCoreNLP(annotators=["wseg", "pos", "ner", "parse"], save_dir=VNCORENLP_DIR)

    # Advanced filter (khÃ´ng SBERT, khÃ´ng NLI, khÃ´ng contradiction detection)
    filter_sys = AdvancedDataFilter(use_sbert=False, use_contradiction_detection=False, use_nli=False)

    total_raw = total_beam = total_final = 0
    simple_outputs, detailed_outputs = [], []

    for idx, sample in enumerate(samples):
        print(f"\nğŸ‘‰ Sample {idx+1}/{total_to_process}")
        s_res, d_res, r_raw, r_beam, r_final = process_sample(
            sample, model, filter_sys, args.min_relevance,
            args.beam_width, args.max_depth, args.max_paths,
            args.max_final_sentences, beam_sentences=args.beam_sentences)
        simple_outputs.append(s_res)
        detailed_outputs.append(d_res)
        total_raw   += r_raw
        total_beam  += r_beam
        total_final += r_final

        if (idx + 1) % 50 == 0:
            print(f"  -> {idx + 1} samples processed ...")

        # ğŸ‘‰ Bá» ghi tá»«ng dÃ²ng JSONL Ä‘á»ƒ quay láº¡i ghi má»™t láº§n cuá»‘i â€“ giá»¯ bá»™ nhá»› á»Ÿ má»©c cháº¥p nháº­n Ä‘Æ°á»£c

    # ğŸ“ Ghi danh sÃ¡ch output (Ä‘á»‹nh dáº¡ng JSON array)
    with open(simple_file, "w", encoding="utf-8") as f:
        json.dump(simple_outputs, f, ensure_ascii=False, indent=2)
    with open(detailed_file, "w", encoding="utf-8") as f:
        json.dump(detailed_outputs, f, ensure_ascii=False, indent=2)
    # --- Ghi file thá»‘ng kÃª tá»•ng ---
    run_stats = {
        "total_context_sentences": total_raw,
        "total_beam_sentences":    total_beam,
        "total_final_sentences":   total_final,
        "num_samples":             total_to_process,
        "beam_parameters": {
            "beam_width": args.beam_width,
            "max_depth":  args.max_depth,
            "max_paths":  args.max_paths,
            "beam_sentences": args.beam_sentences
        }
    }
    with open(stats_file, "w", encoding="utf-8") as f:
        json.dump(run_stats, f, ensure_ascii=False, indent=2)

    print("\n================= Tá»”NG Káº¾T =================")
    print(f"Tá»•ng cÃ¢u sau tÃ¡ch: {total_raw}")
    print(f"Sau Beam Search:   {total_beam}")
    print(f"Sau Lá»c nÃ¢ng cao:  {total_final}")
    print("===========================================")

    print(f"âœ… Done! Output saved to:\n   â€¢ {simple_file}\n   â€¢ {detailed_file}\n   â€¢ {stats_file}")


if __name__ == "__main__":
    main() 